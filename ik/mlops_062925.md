d`avid allen

100 diff't sources

pipeline reads them, filters, transform
- final location (prob a database)

"branching
- put into a different location

different log sources
- procedural way
- doing transformation
- putting into snowflake

Data Pipeline
- Tweet or post
- lands on a server
- go to server that fans it out
  - so others can read your tweet
- put into a pipeline
  - process the tweet
  - generate more data from pipeline
    - count capitalized letters
      - screaming feature
    - tokenization
    - sentiment analysis
    - emojis
    - csv - normalize to json
  - push to storage
    - big query
    - snowflake
    - hdfs
      - distributed file system
        - good for other training workflows
- ETL - extract / transform / load
  - what happens when we are streaming
  - came from batching

- Tools to orchestra
  - repeatable processes and components

- constantly loading from S3

- Apache NiFi
  - Snowflake took ownership

- DataFlows
  - Google

- AWS
  - Glue

- Airflow
  - consistent processes

Snowflake is a dataware house
- can store json blobs
  - can still those quickly
- cloud based database
- fast and easy to query and build against
- Jupyter notebooks

AWS
- many db offerings

Fraud detection examples
- log user authentication, financial, email, web pages, logs
- extract: gather data from various sources
- transform: validation and join
- load: datalake for further analysis
  - database: model pulling from SQL db
  - data warehouse
  - data lake
- gdpr - european data protection 

Databricks:
- medallion
- data architecture
  - null values, masking
  - save raw

---

Raw
Intermediate  - clean up, null masking
Primary       - joined
Feature       - counts
Model input
Models
Model output  -- serving
Reporting     -- loop back up

ML engineering frameworkds
Kedro
- discreet steps
- worry about efficiency later

---

Airflow

Airbnb
: programmatical
  - flexible
: author
: schedule
: monitor
: orchestrator

---

Orchestrator
: everyone is on time
: everyone doing their part
: reliability

---

Airflow
: open source
: flexible
: extensible
: scalable

Competitors
luigi - python (pipes)
mlflow - databricks (open source)

---

Airflow features
: task management
  - documenting tasks
  - showing them around 
: task scheduling
  - retries
  - reporting
: task execution
  - executors
: monitoring and logging

---

Airflow Architecture
: User interface / Webserver
: metadata database
  - management features
  - schedule
: scheduler
: executor
: workers

---

Airflow Scheduler
: when a task should runs

---

Executor
: where the code is going to run

Type of executors
- sequential
- local
- celery : is a message queue
  - allow distribute to different nodes
  - workers can work on items from queue
- dask
  - parallel computing library
- kubernetes
  - executor
  - containers

---

Airflow UI / Webserver
- look at dags
- look at cluster

---

Airflow database

---

Q: k8s vs airflow

airflow:
- workflow orchestration
  - schedule and monitor data pipeline

k8:
- container orchestration
  - automate deployment, scaling, managing containerized apps
 
---

Q: what is stored in airflow db

1. dag def and run history
2. task instances and states
3. variables and connections
4. scheduler information
5. user authentication data

---

Q: workers are processes or VMs

Either

1. simpler deployments, run as processes on same machine
   - LocalExecutor
   - SequentialExecutor
2. complex deployments
   - celeryExecutor
     - workers run as processes distributed across multiple machines
     - each worker is a separate process that pulls taks from celery message queue
     - requires message broker, Redis or RabbitMQ

---

Q: representation of dag?

1. airflow db tracks task dependencies and current state

---

Q: what is the abstraction for the executor?

1. k8 

---

Tasks in Airflow

Operator: BuiltIn

- fundamental unit of execution
- can be common task
  - @task {}
  - operator
 
---
none
- picked up scheduler

scheduled
- picked up by executor
queued
- picked up by worker
running

success

errors:
marked failed
- failed 
error 
- eligible for retry
  - up_for_retry
change
- restart

---

type of tasks in airflow

- operators
- sensors
- @task

---

operator

- template

---

Scheduled Execution

---

Dynamic configuration

- don't change
- make sure that it can handle change in configuration

---

Non-idempotent tasks

- tasks are not designed to be idempotent
- unexpected results during retries

---

shared python environment

- all dags share a common python environment
- isolate environments

---

breadth

MLOps
- specialized in MLOps
- serving / training
- building models / feature engineering
- where you want to go
- prune out / going deeper into some of these areas
- broad of understanding of terms and what things are
- what it is good for and not what it is good for
- don't need to know the internals that well
- broad / shallow understanding
  - entire lifecyle class of tools
  - rather than trying to understand all of this very deeply

IK
- optimizations
- TensorRT
- model parallelism
- cuda kernels

MLOps
- no job posting for ML ops
- looking someone for serving path and inference path
 
