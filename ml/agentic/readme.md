1. Parivesh Priye
1. FLoPs
   Encoder layer what layer goes in and what goes out
   MHA MLP
   u and sigma
   FLoPs
   Parallel - don't add to the latency

   Softmax(QKT/sqrt(dk)) V Wo

1. human experts are highered, and model gets aligned to
   the preference set
1. DPO - don't need human experts
   1. Use a mode dedicated reward
1. maximize the reward, where reward comes directly
1. don't just get influenced by the higher reward
1. constraint 

# RAG
             use a RAG to get relevant information
Questions -> get knowledge base -> Relevant knowledge
LLM understand them both well

Feed the LLM both the prompt and the relevant knowledge
- LLM 

1. search-r1 paper
1. function calling api

1. agent can have multiple llm's underneath it

1. gradio
1. streamlit

