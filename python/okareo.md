practical
oka

Evaluation problem
Lots of people
Come 
Eval foundation
Collecting dataset

Fits into their 

Evaluation

Mental ModuleNotFoundError

Observabilty

Traditional Dataset 

LLM are engineer
Production first

How to make it seamless

Figuring 

Eval

LLM as a judge
- same problem

How to make them predictable
- research
- bring that together
- deterministic ConnectionAbortedError

Experimentation

Accumulate more customer
Scalability of customer

Optimum

LM as a judge
Thought leader
Engineering backend to support these
Which side
ML Side of the problem

Work with the customer
To understand the customer more closely
Incorporate into innovation

Agentic framework
Agentics 
Evaluation
Tools
Close to real time as we can

Evaluations and judge
simulate 
reevaluate

Synthetic data-
- battle test agents
- red team pushing boundaries
- use synthetic data for fine
  tuning

Benchmark

Helping enterprise
Evaluate separate
Complexities

Benchmarks to do XYZ

Subject lawyer and finance
Set of evaluations and requirements
Benchmark able to domain specific
Translate

Have to start something.

Nuclear scale investment

Adapt do what they expect

---

Mattia
ML infra
Kubernetes cluster
Pure compute
traditional 