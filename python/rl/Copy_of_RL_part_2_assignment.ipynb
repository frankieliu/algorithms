{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYwPYBDlced0"
      },
      "source": [
        "In this assignment, we will use\n",
        "* causality return\n",
        "* value baseline\n",
        "\n",
        "to make policy gradient update in the Lunar lander environment.\n",
        "\n",
        "You can modify from the trainer implementation in [class demo](https://drive.google.com/file/d/1MmlBMyiFZos_16YzoiWKbCkOZs_fOR12/view).\n",
        "* Complete the return and advantage implementation.\n",
        "* Update the trainer to call appropriate return/advantage computation for policy gradient updates.\n",
        "* Implement the critic network, its loss function and back propagation logic.\n",
        "\n",
        "Reinforcement learning is sensitive to hyperparameters. Lunar Lander environment is considered solved with 200 episode return. Our simple implementations should generally be able to reach > 100 returns. Test hyperparameters such as learning rate, max episode length, number of epochs, number of steps per epoch, return/advantage computation to see what tweeks help.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8fqL1z7xOgkO"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies for LunarLander.\n",
        "# !apt-get install swig > /dev/null 2>&1\n",
        "# !pip install gymnasium > /dev/null 2>&1\n",
        "# !pip install gymnasium[box2d] > /dev/null 2>&1\n",
        "# !pip install gymnasium[classic-control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fGCUizxlOSbc"
      },
      "outputs": [],
      "source": [
        "#@title Import required libraries.\n",
        "\n",
        "# import pandas as pd\n",
        "# import gymnasium as gym\n",
        "# import numpy as np\n",
        "# import random\n",
        "# import seaborn as sns\n",
        "\n",
        "# from IPython import display\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.distributions.categorical import Categorical\n",
        "# from torch.optim import Adam\n",
        "# import numpy as np\n",
        "# from gymnasium.spaces import Discrete, Box\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRGbwtwDuFu1"
      },
      "outputs": [],
      "source": [
        "#@title Add your implementation.\n",
        "def reinforce_returns(episode_rewards):\n",
        "    \"\"\"episode_rewards: list of reward at each time stamp in an episode.\n",
        "\n",
        "    This function should compute returns for all timestamps as sum of all\n",
        "    rewards in the episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def causality(episode_rewards):\n",
        "    \"\"\"episode_rewards: list of reward at each time stamp in an episode.\n",
        "\n",
        "    This function should compute returns as sum of rewards at current and later\n",
        "    timestamps in the episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def compute_advantage(episode_returns, episode_values):\n",
        "    \"\"\"episode_rewards: list of returns/cumulative reward at each time stamp in\n",
        "           an episode.\n",
        "       episode_values: tensor (or list) of predicted state value at each time\n",
        "           stamp in an episode.\n",
        "\n",
        "    This function should compute a advantage, using computed returns and\n",
        "    estimated state values.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DiJKqdsSf1H"
      },
      "outputs": [],
      "source": [
        "# Trains a policy using policy gradient update.\n",
        "class PolicyGradientTrainer():\n",
        "    def __init__(self, env_name: str = 'LunarLander-v2', lr: float = 1e-2,\n",
        "                 value_lr: float = 1e-3, max_eps_length: int = 500,\n",
        "                 epochs: int = 50, n_steps_per_epoch: int = 5000,\n",
        "                 hidden_sizes: list[int] = [32],\n",
        "                 hidden_sizes2: list[int] = [32], method: str = 'reinforce',\n",
        "                 verbose: bool = False):\n",
        "        if method not in {'reinforce', 'causality', 'advantage'}:\n",
        "            raise ValueError('Unsupported method.')\n",
        "\n",
        "    # Train the policy model and return training statistics.\n",
        "    def train(self):\n",
        "        pass\n",
        "\n",
        "    # Evaluate the policy model by acting in the environment for one episode.\n",
        "    def eval(self, render_every: int = 1):\n",
        "        pass\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
